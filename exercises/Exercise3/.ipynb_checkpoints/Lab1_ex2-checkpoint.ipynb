{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning on MCU - LAB1 ex2\n",
    "Predicting Human Activity from Smartphone Accelerometer and Gyroscope Data\n",
    "\n",
    "We use a dataset of 3-axial accelerometer signals from an academic experiment on the UC Irvine Machine Learning Repository.\n",
    "\n",
    "The dataset can be downloaded here: https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions#\n",
    "where you can also find the description of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If required, download the dataset\n",
    "import requests\n",
    "import os.path\n",
    "import zipfile\n",
    "if (not os.path.isdir('./HAPT Data Set')):\n",
    "    open('./HAPT Data Set.zip', 'wb').write(requests.get(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/00341/HAPT%20Data%20Set.zip\", \n",
    "        allow_redirects=True).content)\n",
    "    zipfile.ZipFile('./HAPT Data Set.zip', 'r').extractall('./HAPT Data Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "random.seed(7)\n",
    "\n",
    "# display pandas results to 3 decimal points, not in scientific notation\n",
    "# pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the accelerometer and gyroscope data.\n",
    "We read the feature names from features.txt and the activity labels from activity_labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./HAPT Data Set/features.txt') as f:\n",
    "    features = f.read().split()\n",
    "\n",
    "print('There are {} features.'.format(len(features)))\n",
    "    \n",
    "with open('./HAPT Data Set/activity_labels.txt') as f:\n",
    "    activity_labels = f.readlines()\n",
    "\n",
    "activity_df = [x.split() for x in activity_labels]\n",
    "print('There are {} activities.'.format(len(activity_df)))\n",
    "pd.DataFrame(activity_df, columns = ['Activity_id', 'Activity_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are pre-split into training and test sets. Let's load the features x and the labels y, and have a look at a few features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_table('./HAPT Data Set/Train/X_train.txt',\n",
    "             header = None, sep = \" \", names = list(dict.fromkeys(features)))\n",
    "X_train.iloc[:10, :10].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_table('./HAPT Data Set/Train/y_train.txt',\n",
    "             header = None, sep = \" \", names = ['Activity_id'])\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_table('./HAPT Data Set/Test/X_test.txt',\n",
    "             header = None, sep = \" \", names = list(dict.fromkeys(features)))\n",
    "y_test = pd.read_table('./HAPT Data Set/Test/y_test.txt',\n",
    "             header = None, sep = \" \", names = ['Activity_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human activity classification\n",
    "Now that weâ€™ve loaded the train and test data into memory, we can start building a model to predict the activity from the features. We use the python library scikit-learn.\n",
    "\n",
    "The test set must not be used until the very last step of testing (as the name suggests). So we split the given training set into training and validation sets. The training set is used for training and the validation set is used for validating the models (identifying the best model, tuning the hyperparameters).\n",
    "\n",
    "In this exercise we'll use 5-fold cross-validation (CV), which means that the data are randomly partitioned into 5 equal-sized sub-datasets, of which 1 sub-dataset is retained as the validation data for testing the model, and the remaining 4 are used for training. The whole process is repeated 5 times, with each of the 5 sub-datasets used exactly once as the validation data. The results can then be averaged to produce a single estimation.\n",
    "\n",
    "Let's start with a Linear Support Vector Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Declare the hyper-parameter\n",
    "C_params = np.logspace(-6, 3, 10)\n",
    "\n",
    "# Declare the classfier\n",
    "clf_svc = LinearSVC(random_state = 7)\n",
    "\n",
    "# Compute training and test scores for varying parameter values\n",
    "train_scores, val_scores = validation_curve(\n",
    "    clf_svc, X_train.values, y_train.values.flatten(),\n",
    "    param_name = \"C\", param_range = C_params,\n",
    "    cv = 5, scoring = \"accuracy\", n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traning scores and the validation scores are saved. Now we can plot the learning curves wrt the parameter. Let's first calculate the means and the standard deviations of the validation, and then plot the training and validation accuracy vs. parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(train_scores, val_scores, C_params):\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    val_scores_mean = np.mean(val_scores, axis=1)\n",
    "    val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "    # To better visualize the plot, we set the y-axis limits\n",
    "    y_min = 0.5\n",
    "    y_max = 1.0\n",
    "\n",
    "    f = plt.figure(figsize = (12, 8))\n",
    "    ax = plt.axes()\n",
    "    plt.title(\"SVM Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"C Value\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.yticks(np.arange(y_min, y_max + .01, .05))\n",
    "    plt.semilogx(C_params, train_scores_mean, label = \"Training Accuracy\", color = \"red\")\n",
    "    plt.fill_between(C_params, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha = 0.2, color = \"red\")\n",
    "    plt.semilogx(C_params, val_scores_mean, label = \"Validation Accuracy\",\n",
    "                 color = \"green\")\n",
    "    plt.fill_between(C_params, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha = 0.2, color = \"green\")\n",
    "    plt.legend(loc = \"best\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_accuracy(train_scores, val_scores, C_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, it looks like the best value of C is at 0.1. The validation accuracy begins slowly decreasing after that 0.1, indicating that we are starting to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean validation score is {:.4f}\".format(np.mean(val_scores[np.where(C_params == 0.1)[0][0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we've included all the features, will a subset of features perform better than including all of them? Let's find it out.\n",
    "\n",
    "One way would be to select particular features manually, let's look at the features list: (you can also open features.txt in a text editor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 40 features are derived from time domain accelerometer 3-axial signals (for more details read features_info.txt). Let's do the same using only these 40 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = X_train.iloc[:,0:40]\n",
    "X_test_new = X_test.iloc[:,0:40]\n",
    "\n",
    "# Compute training and test scores for varying parameter values\n",
    "train_scores, val_scores = validation_curve(\n",
    "    clf_svc, X_train_new.values, y_train.values.flatten(),\n",
    "    param_name = \"C\", param_range = C_params,\n",
    "    cv = 5, scoring = \"accuracy\", n_jobs = -1)\n",
    "\n",
    "plot_accuracy(train_scores, val_scores, C_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier performs worse (as expected, since the feature set is not well selected).\n",
    "\n",
    "The manual selection of subset and combinations of subsets of features could be extremely cumbersome. Let's use a more advanced function in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "feature_selector = SelectKBest(k=20).fit(X_train.values, y_train.values.flatten())\n",
    "X_train_new = feature_selector.transform(X_train.values)\n",
    "X_test_new = feature_selector.transform(X_test.values)\n",
    "# We use .values because X_train is a panda dataset\n",
    "# The output of .transform is an array, therefore we don't need to use .values anymore in the validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training and test scores for varying parameter values\n",
    "train_scores, val_scores = validation_curve(\n",
    "    clf_svc, X_train_new, y_train.values.flatten(),\n",
    "    param_name = \"C\", param_range = C_params,\n",
    "    cv = 5, scoring = \"accuracy\", n_jobs = -1)\n",
    "\n",
    "plot_accuracy(train_scores, val_scores, C_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 20 best features (best according to ANOVA -- it's the default score function in SelectKBest, since we didn't specify any score_func) are not enough and the performance is worse than including all the features.\n",
    "\n",
    "(Optional) Try with higher k to see if there is a subset of features which can perform better than including all the features.\n",
    "\n",
    "The parameter k in SelectKBest is another parameter which should be tuned. In the documentation of the function you can find also a list of score function which can be used. As you can notice, the options are broad. Due to time limitations, we don't investigate further into feature selection during the lab session. In the following steps of this exercise we will use the feature matrices X_train and X_test including all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of linear SVM is already quite good, but we can investigate further and do a grid serach on different kernels and C values for the SVM model. With a larger search space, we might get a different set of optimal parameters.\n",
    "\n",
    "Use the general Support Vector Classifier (SVC) in scikit-learn and try non-linear kernels. Use GridSearchCV in model_selection to search for the best parameters. (Read the documentation pages of SVC and GridSearchCV on scikit-learn, there you also find example codes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the functions SVC and GridSearchCV\n",
    "\n",
    "\n",
    "# Declare the parameters\n",
    "# (Pay attention to the number of paramters you declare, because more points in the GridSearch will cost you longer training time -- could take more than half an hour...)\n",
    "\n",
    "\n",
    "# Declare the classifier (estimator) to be used in GridSearchCV\n",
    "\n",
    "\n",
    "# Declare the classifier using GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifier and find the best parameters using GridSearchCV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which one is the best model and the best parameter? What's the best CV score you got?\n",
    "# (Hint: read the list of Attributes in the documentation page of GridSearchCV on scikit-learn)\n",
    "\n",
    "\n",
    "# Predict on test data using the best model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a general understanding of training, validating, and testing using the python library scikit-learn. The following points are optional, in case you want to explore more about the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try other classifiers, for example Decision Trees, Nearest Neighbors, or Ensemble methods. Can you get better results?\n",
    "\n",
    "# After identifying the best model with the optimal set of parameters, test your model using the test set. \n",
    "\n",
    "# What's your test accuracy? Are you satisfied? How complex is your model to be deployed on a microcontroller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
